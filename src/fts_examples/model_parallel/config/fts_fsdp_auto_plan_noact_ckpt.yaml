trainer:
  max_epochs: 2  # 4
  callbacks+:
  - class_path: finetuning_scheduler.FinetuningScheduler
    init_args:
      ft_schedule: ./config/defaults/ModParallelExample_ft_schedule.yaml
      max_depth: 2
      strategy_adapter_cfg:
        fsdp_plan: {"model.output": {}, 'model.layers.\d*$': {}}
        fsdp_default_kwargs:
          cpu_offload_policy: {}
  strategy:
    init_args:
      data_parallel_size: 2
      tensor_parallel_size: 1
  logger:
    init_args:
      name: fts_fsdp_auto_plan_larger_noact_ckpt
model:
  init_args:
    model_cfg: torchtitan_llama_larger_act.yaml
