# Python package
# Create and test a Python package on multiple Python versions.
# Add steps that analyze code, save the dist with the build record, publish to a PyPI-compatible index, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
  # tags:
  #   include:
  #     - '*'
  branches:
    include:
      - "main"
      - "release/*"
      - "refs/tags/*"
pr:
  branches:
    include:
    - main
    - release/*

jobs:
  - job: pytest
    # how long to run the job before automatically cancelling
    timeoutInMinutes: "20"
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: "2"

    pool: default

    container:
      # run on torch 1.8 as it's the LTS version
      image: "speediedan/finetuning-scheduler:py3.9-pt1.8-azpl-init"
      mapDockerSocket: false
      volumes:
      - /var/run/user/998/docker.sock:/var/run/docker.sock
      options: --gpus all

    workspace:
      clean: outputs

    steps:
    - bash: |
        pip install . --requirement requirements/devel.txt
      displayName: 'Install dependencies'

    - bash: |
        python requirements/collect_env_details.py
        python -c "import torch ; mgpu = torch.cuda.device_count() ; assert mgpu >= 2, f'GPU: {mgpu}'"
      displayName: 'Env details'

    # - bash: |
    #     python -m coverage run --source finetuning_scheduler -m pytest finetuning_scheduler tests -v --junitxml=$(Build.Repository.LocalPath)/test-results.xml --durations=50
    #   displayName: 'Testing: standard'

    - bash: |
        bash ./tests/standalone_tests.sh -k test_fts_multi_
      displayName: 'Testing: standalone multi-gpu'

    - bash: |
        python -m coverage report
        python -m coverage xml
        python -m coverage html
        python -m codecov --commit=$(Build.SourceVersion) --flags=gpu,pytest --name="GPU-coverage" --env=linux,azure
      displayName: 'Statistics'

    # - task: PublishTestResults@2
    #   displayName: 'Publish test results'
    #   inputs:
    #     testResultsFiles: '$(Build.Repository.LocalPath)/test-results.xml'
    #     testRunTitle: '$(Agent.OS) - $(Build.DefinitionName) - Python $(python.version)'
    #   condition: succeededOrFailed()

    - script: |
        set -e
        python -m pytest fts_examples -v --maxfail=1 --durations=0
      displayName: 'Testing: examples'

    # - script: |
    #     python fts_examples/fts_superglue.py fit --config fts_examples/config/fts_explicit.yaml
    #   displayName: 'Testing: full explicit example benchmark'
